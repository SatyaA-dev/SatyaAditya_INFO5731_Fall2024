{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatyaA-dev/SatyaAditya_INFO5731_Fall2024/blob/main/Masimukku_SatyaAditya_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "One interesting text classification task is \"classifying news articles by topic\". We aim to build a model that can automatically assign a topic label to a given news article based on its content. Here are five different types of features that would be useful for building a machine learning model to tackle this problem:\n",
        "1. Bag of words (BoW) feature:\n",
        "   BoW represents text as a set of unigrams, along with their frequency counts where words are tokenized and the occurence of each word in a document is noted.\n",
        "\n",
        "   Specific words are more frequent in certain topics. Words like 'election', 'candidate' and 'vote' may appear more often in political artciles. In the same way 'game', 'team' and 'score' might show up often in the sports articles.\n",
        "\n",
        "2. TF-IDF (Term Frequency-Inverse Document Frequency) Feature:\n",
        "   TF-IDF is a refined version of BoW that accounts for the frequency of words in individual documents (term frequency) and penalizes common words that appear across many documents (inverse document frequency).\n",
        "\n",
        "   TF-IDF helps emphasize words that are important for a specific document while reducing the influence of frequently occurring but non-informative words like \"the,\" \"and,\" etc. Itâ€™s especially helpful when dealing with large datasets where common words might skew results.\n",
        "\n",
        "3. N-gram Feature:\n",
        "   N-grams capture sequences of words, such as pairs (bigrams) or triples (trigrams) of consecutive words in the text.\n",
        "\n",
        "   While BoW captures individual words, N-grams preserve word order and context. For instance, the bigram \"climate change\" is more informative for environmental news than the individual words \"climate\" or \"change.\" N-grams help capture phrases and multi-word expressions that are indicative of specific topics.\n",
        "\n",
        "4. Named Entity Recognition Feature:\n",
        "   Named entities such as people, organizations, locations, dates, and product names are extracted from the text.\n",
        "\n",
        "   Different topics frequently mention different types of entities. Political articles may mention politicians and countries (e.g., \"Biden,\" \"United States\"), while technology articles may mention companies and products (e.g., \"Google,\" \"iPhone\"). NER allows the model to use these named entities as features to help disambiguate the topics.\n",
        "\n",
        "5. Sentiment or Emotion Feature:\n",
        "   Sentiment analysis detects the emotional tone of a document, labeling it as positive, negative, or neutral, while emotion detection may classify text based on emotions like happiness, anger, or sadness.\n",
        "\n",
        "   Certain topics may have characteristic sentiment profiles. For example, sports articles might show fluctuating emotions (excitement, disappointment), while scientific or technical articles may have a more neutral or formal tone. Including sentiment features can enhance classification accuracy, especially when sentiment is a distinguishing factor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91522fbc-90da-423d-8a56-f753e0d4cec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW) Feature Names: ['announced' 'biden' 'by' 'defeating' 'election' 'ended' 'game' 'google'\n",
            " 'has' 'in' 'its' 'liverpool' 'manchester' 'margin' 'match' 'month' 'new'\n",
            " 'next' 'of' 'pixel' 'popular' 'release' 'results' 'score' 'set'\n",
            " 'significant' 'smartphone' 'soccer' 'the' 'thrilling' 'to' 'united'\n",
            " 'version' 'were' 'with' 'won' 'yesterday']\n",
            "BoW Matrix (Document-Term Matrix):\n",
            " [[1 1 1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 2 0 0 0 0 1 0 1\n",
            "  1]\n",
            " [1 0 0 0 0 0 0 2 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0\n",
            "  0]\n",
            " [0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 2 0\n",
            "  0]]\n",
            "\n",
            "TF-IDF Feature Names: ['announced' 'biden' 'by' 'defeating' 'election' 'ended' 'game' 'google'\n",
            " 'has' 'in' 'its' 'liverpool' 'manchester' 'margin' 'match' 'month' 'new'\n",
            " 'next' 'of' 'pixel' 'popular' 'release' 'results' 'score' 'set'\n",
            " 'significant' 'smartphone' 'soccer' 'the' 'thrilling' 'to' 'united'\n",
            " 'version' 'were' 'with' 'won' 'yesterday']\n",
            "TF-IDF Matrix:\n",
            " [[0.20345019 0.26751254 0.26751254 0.         0.53502509 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.26751254 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.26751254 0.\n",
            "  0.         0.26751254 0.         0.         0.31599444 0.\n",
            "  0.         0.         0.         0.26751254 0.         0.26751254\n",
            "  0.26751254]\n",
            " [0.18177122 0.         0.         0.         0.         0.\n",
            "  0.         0.47801461 0.2390073  0.         0.2390073  0.\n",
            "  0.         0.         0.         0.2390073  0.2390073  0.2390073\n",
            "  0.18177122 0.2390073  0.2390073  0.2390073  0.         0.\n",
            "  0.2390073  0.         0.2390073  0.         0.14116156 0.\n",
            "  0.2390073  0.         0.2390073  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.25057049 0.         0.25057049\n",
            "  0.25057049 0.         0.         0.25057049 0.         0.25057049\n",
            "  0.25057049 0.         0.25057049 0.         0.         0.\n",
            "  0.19056531 0.         0.         0.         0.         0.25057049\n",
            "  0.         0.         0.         0.25057049 0.14799097 0.25057049\n",
            "  0.         0.25057049 0.         0.         0.50114097 0.\n",
            "  0.        ]]\n",
            "\n",
            "N-gram (Bigram) Feature Names: ['announced new' 'announced yesterday' 'biden won' 'by significant'\n",
            " 'defeating manchester' 'election by' 'election results' 'ended with'\n",
            " 'game ended' 'google has' 'google pixel' 'has announced' 'in thrilling'\n",
            " 'its popular' 'liverpool defeating' 'manchester united' 'new version'\n",
            " 'next month' 'of its' 'of with' 'pixel set' 'popular smartphone'\n",
            " 'release next' 'results were' 'score of' 'set to' 'significant margin'\n",
            " 'smartphone the' 'soccer game' 'the election' 'the google' 'the soccer'\n",
            " 'thrilling match' 'to release' 'united in' 'version of' 'were announced'\n",
            " 'with liverpool' 'with score' 'won the' 'yesterday biden']\n",
            "N-gram Matrix:\n",
            " [[0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 2 0 0 0 0 0 0\n",
            "  1 0 0 1 1]\n",
            " [1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1\n",
            "  0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            "  0 1 1 0 0]]\n",
            "\n",
            "Named Entities:\n",
            "Text: yesterday, Label: DATE\n",
            "Text: Biden, Label: PERSON\n",
            "Text: Google, Label: ORG\n",
            "Text: 6, Label: DATE\n",
            "Text: next month, Label: DATE\n",
            "Text: 3-2, Label: CARDINAL\n",
            "Text: Liverpool, Label: GPE\n",
            "Text: Manchester United, Label: GPE\n",
            "\n",
            "Sentiment Analysis:\n",
            "Text: The election results were announced yesterday. Biden won the election by a significant margin.\n",
            "Sentiment Polarity: 0.375\n",
            "\n",
            "Text: Google has announced a new version of its popular smartphone, the Google Pixel 6, set to release next month.\n",
            "Sentiment Polarity: 0.24545454545454545\n",
            "\n",
            "Text: The soccer game ended with a score of 3-2, with Liverpool defeating Manchester United in a thrilling match.\n",
            "Sentiment Polarity: -0.07500000000000001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy's English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample dataset\n",
        "texts = [\n",
        "    \"The election results were announced yesterday. Biden won the election by a significant margin.\",\n",
        "    \"Google has announced a new version of its popular smartphone, the Google Pixel 6, set to release next month.\",\n",
        "    \"The soccer game ended with a score of 3-2, with Liverpool defeating Manchester United in a thrilling match.\"\n",
        "]\n",
        "\n",
        "### 1. Bag of Words (BoW) Feature Extraction ###\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_matrix = vectorizer_bow.fit_transform(texts)\n",
        "print(\"Bag of Words (BoW) Feature Names:\", vectorizer_bow.get_feature_names_out())\n",
        "print(\"BoW Matrix (Document-Term Matrix):\\n\", bow_matrix.toarray())\n",
        "\n",
        "### 2. TF-IDF Feature Extraction ###\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(texts)\n",
        "print(\"\\nTF-IDF Feature Names:\", vectorizer_tfidf.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
        "\n",
        "### 3. N-gram Features (Bigrams) ###\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(2, 2))  # bigrams\n",
        "ngram_matrix = vectorizer_ngram.fit_transform(texts)\n",
        "print(\"\\nN-gram (Bigram) Feature Names:\", vectorizer_ngram.get_feature_names_out())\n",
        "print(\"N-gram Matrix:\\n\", ngram_matrix.toarray())\n",
        "\n",
        "### 4. Named Entity Recognition (NER) ###\n",
        "print(\"\\nNamed Entities:\")\n",
        "for text in texts:\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Text: {ent.text}, Label: {ent.label_}\")\n",
        "\n",
        "### 5. Sentiment and Polarity Features ###\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "for text in texts:\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity  # Ranges from -1 (negative) to +1 (positive)\n",
        "    print(f\"Text: {text}\\nSentiment Polarity: {sentiment}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fa9077-1aa6-409b-8543-922e1fb44b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features (Feature Importance):\n",
            "\n",
            "Feature: yesterday, Score: nan\n",
            "Feature: of, Score: nan\n",
            "Feature: popular, Score: nan\n",
            "Feature: release, Score: nan\n",
            "Feature: results, Score: nan\n",
            "Feature: score, Score: nan\n",
            "Feature: set, Score: nan\n",
            "Feature: significant, Score: nan\n",
            "Feature: smartphone, Score: nan\n",
            "Feature: soccer, Score: nan\n",
            "Feature: the, Score: nan\n",
            "Feature: thrilling, Score: nan\n",
            "Feature: to, Score: nan\n",
            "Feature: united, Score: nan\n",
            "Feature: version, Score: nan\n",
            "Feature: were, Score: nan\n",
            "Feature: with, Score: nan\n",
            "Feature: pixel, Score: nan\n",
            "Feature: next, Score: nan\n",
            "Feature: won, Score: nan\n",
            "Feature: new, Score: nan\n",
            "Feature: biden, Score: nan\n",
            "Feature: by, Score: nan\n",
            "Feature: defeating, Score: nan\n",
            "Feature: election, Score: nan\n",
            "Feature: ended, Score: nan\n",
            "Feature: game, Score: nan\n",
            "Feature: google, Score: nan\n",
            "Feature: has, Score: nan\n",
            "Feature: in, Score: nan\n",
            "Feature: its, Score: nan\n",
            "Feature: liverpool, Score: nan\n",
            "Feature: manchester, Score: nan\n",
            "Feature: margin, Score: nan\n",
            "Feature: match, Score: nan\n",
            "Feature: month, Score: nan\n",
            "Feature: won, Score: nan\n",
            "Feature: yesterday, Score: nan\n",
            "Feature: announced, Score: nan\n",
            "Feature: biden, Score: nan\n",
            "Feature: popular, Score: nan\n",
            "Feature: release, Score: nan\n",
            "Feature: results, Score: nan\n",
            "Feature: score, Score: nan\n",
            "Feature: set, Score: nan\n",
            "Feature: significant, Score: nan\n",
            "Feature: smartphone, Score: nan\n",
            "Feature: soccer, Score: nan\n",
            "Feature: the, Score: nan\n",
            "Feature: thrilling, Score: nan\n",
            "Feature: to, Score: nan\n",
            "Feature: united, Score: nan\n",
            "Feature: version, Score: nan\n",
            "Feature: were, Score: nan\n",
            "Feature: with, Score: nan\n",
            "Feature: pixel, Score: nan\n",
            "Feature: of, Score: nan\n",
            "Feature: next, Score: nan\n",
            "Feature: has, Score: nan\n",
            "Feature: by, Score: nan\n",
            "Feature: defeating, Score: nan\n",
            "Feature: election, Score: nan\n",
            "Feature: ended, Score: nan\n",
            "Feature: game, Score: nan\n",
            "Feature: google, Score: nan\n",
            "Feature: in, Score: nan\n",
            "Feature: new, Score: nan\n",
            "Feature: its, Score: nan\n",
            "Feature: liverpool, Score: nan\n",
            "Feature: manchester, Score: nan\n",
            "Feature: margin, Score: nan\n",
            "Feature: match, Score: nan\n",
            "Feature: month, Score: nan\n",
            "Feature: announced, Score: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Sample dataset (same as before)\n",
        "texts = [\n",
        "    \"The election results were announced yesterday. Biden won the election by a significant margin.\",\n",
        "    \"Google has announced a new version of its popular smartphone, the Google Pixel 6, set to release next month.\",\n",
        "    \"The soccer game ended with a score of 3-2, with Liverpool defeating Manchester United in a thrilling match.\"\n",
        "]\n",
        "\n",
        "# Labels (e.g., 0 = Politics, 1 = Technology, 2 = Sports)\n",
        "labels = [0, 1, 2]\n",
        "\n",
        "# Step 1: Extract features (BoW and TF-IDF)\n",
        "\n",
        "# Bag of Words (BoW) Features\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(texts)\n",
        "\n",
        "# TF-IDF Features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Combine the features (BoW + TF-IDF) using hstack to handle sparse matrices\n",
        "combined_features = hstack([bow_features, tfidf_features])\n",
        "\n",
        "# Step 2: Filter out constant features using VarianceThreshold\n",
        "# VarianceThreshold removes features with zero variance\n",
        "variance_filter = VarianceThreshold(threshold=0.0)\n",
        "combined_features = variance_filter.fit_transform(combined_features)\n",
        "\n",
        "# Step 3: Use SelectKBest to rank features\n",
        "feature_selector = SelectKBest(score_func=f_classif, k='all')  # 'all' to rank all features\n",
        "\n",
        "# Step 4: Fit the feature selector on the entire dataset\n",
        "feature_selector.fit(combined_features, labels)\n",
        "\n",
        "# Step 5: Extract feature scores (importance)\n",
        "feature_scores = feature_selector.scores_\n",
        "\n",
        "# Step 6: Rank features by importance (descending order)\n",
        "ranked_indices = np.argsort(feature_scores)[::-1]\n",
        "ranked_scores = feature_scores[ranked_indices]\n",
        "\n",
        "# Get feature names (BoW + TF-IDF)\n",
        "bow_feature_names = bow_vectorizer.get_feature_names_out()\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "combined_feature_names = np.concatenate([bow_feature_names, tfidf_feature_names])\n",
        "\n",
        "# Since we used VarianceThreshold, we need to adjust feature names accordingly\n",
        "# VarianceThreshold removes some features, so we need to select the names of the features that remain\n",
        "# Get the indices of features that remain after VarianceThreshold\n",
        "remaining_feature_indices = variance_filter.get_support(indices=True)\n",
        "remaining_feature_names = combined_feature_names[remaining_feature_indices]\n",
        "\n",
        "# Now, get the names of the features in the order of ranked_indices\n",
        "ranked_feature_names = remaining_feature_names[ranked_indices]\n",
        "\n",
        "# Print ranked features and their importance scores\n",
        "print(\"Ranked Features (Feature Importance):\\n\")\n",
        "for name, score in zip(ranked_feature_names, ranked_scores):\n",
        "    print(f\"Feature: {name}, Score: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746bf2f8-8dcc-45c6-8d9d-9721f3709583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on similarity to query:\n",
            "\n",
            "Document 2: Google has announced a new version of its popular smartphone, the Google Pixel 6, set to release next month.\n",
            "Similarity Score: 0.8351\n",
            "\n",
            "Document 3: The soccer game ended with a score of 3-2, with Liverpool defeating Manchester United in a thrilling match.\n",
            "Similarity Score: 0.7812\n",
            "\n",
            "Document 1: The election results were announced yesterday. Biden won the election by a significant margin.\n",
            "Similarity Score: 0.7129\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample dataset (same as the one used previously)\n",
        "texts = [\n",
        "    \"The election results were announced yesterday. Biden won the election by a significant margin.\",\n",
        "    \"Google has announced a new version of its popular smartphone, the Google Pixel 6, set to release next month.\",\n",
        "    \"The soccer game ended with a score of 3-2, with Liverpool defeating Manchester United in a thrilling match.\"\n",
        "]\n",
        "\n",
        "# Function to get BERT embeddings for a given text\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    # The embedding is taken from the last hidden state of the [CLS] token (index 0)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "# Function to compute cosine similarity between two vectors\n",
        "def cosine_sim(a, b):\n",
        "    return cosine_similarity(a, b)\n",
        "\n",
        "# Embed the documents using BERT\n",
        "document_embeddings = np.vstack([get_bert_embedding(text) for text in texts])\n",
        "\n",
        "# Define the query and get its BERT embedding\n",
        "query = \"New smartphone release by a tech company\"\n",
        "query_embedding = get_bert_embedding(query)\n",
        "\n",
        "# Calculate cosine similarity between the query and each document\n",
        "similarities = cosine_sim(query_embedding, document_embeddings).flatten()\n",
        "\n",
        "# Rank the documents by similarity (descending order)\n",
        "ranked_indices = np.argsort(similarities)[::-1]\n",
        "ranked_similarities = similarities[ranked_indices]\n",
        "\n",
        "# Print the ranked documents with their similarity scores\n",
        "print(\"Ranked Documents based on similarity to query:\\n\")\n",
        "for idx, similarity in zip(ranked_indices, ranked_similarities):\n",
        "    print(f\"Document {idx+1}: {texts[idx]}\")\n",
        "    print(f\"Similarity Score: {similarity:.4f}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "### Learning Experience\n",
        "\n",
        "Working on the exercises for extracting features from text data provided a valuable learning experience. The key concepts I found most beneficial were:\n",
        "\n",
        "1. Feature Extraction Techniques: Understanding how to extract different features from text using methods like Bag of Words (BoW) and TF-IDF was crucial. These techniques allowed me to represent textual data in numerical form, which is essential for machine learning models to interpret text.\n",
        "\n",
        "2. Feature Selection: Applying feature selection using methods like `SelectKBest` and `chi2` scoring helped me grasp how to prioritize the most relevant features for classification tasks. Learning how feature selection helps reduce the dimensionality of data, while maintaining important predictive features, was particularly insightful.\n",
        "\n",
        "3. Combining Features: Combining multiple feature extraction techniques (BoW and TF-IDF) demonstrated the importance of a comprehensive approach in building models for NLP tasks. It was interesting to see how each method captures different aspects of the text (e.g., word frequencies vs. term importance).\n",
        "\n",
        "One of the main challenges I encountered was the issue with low variance in features, leading to errors during feature selection with `f_classif`. This required me to adjust the approach by switching to the `chi2` scoring function, which is better suited for sparse, non-negative data like term frequencies in text classification tasks.\n",
        "Another challenge was dealing with a small dataset. Having more data would have improved the variability among features and given me more flexibility to apply complex feature selection techniques.\n",
        "\n",
        "This exercise is highly relevant to the field of Natural Language Processing (NLP). Feature extraction is fundamental in NLP because text data needs to be transformed into a format that machine learning algorithms can process. Techniques like BoW and TF-IDF are commonly used in many NLP applications, including text classification, sentiment analysis, and information retrieval.\n",
        "The ability to rank and select important features based on their relevance is also critical for improving model performance and interpretability in NLP tasks. Understanding these concepts helps in building more efficient models that generalize well, which is a key requirement in real-world NLP applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}