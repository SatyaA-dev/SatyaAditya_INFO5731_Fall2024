{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatyaA-dev/SatyaAditya_INFO5731_Fall2024/blob/main/SatyaAdityaMasimukku_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Research Question:\n",
        "How does the representation of interdisciplinary collaboration evolve in scientific literature over the past decade?\n",
        "\n",
        "Rationale:\n",
        "Interdisciplinary collaboration is becoming more essential as research becomes more complex. This research would investigate how the collaboration between different fields, e.g., biology and computer science, or social sciences and data science, has evolved over the years. It would also explore how the outcomes of these collaborations differ from traditional, single-discipline research.\n",
        "\n",
        "The goal is to identify trends, emerging interdisciplinary fields, key authors and institutions involved in such collaborations, and the impact of these collaborations on the progression of knowledge.\n",
        "\n",
        "Hypotheses:\n",
        "1. The number of interdisciplinary papers has increased over the past 10 years.\n",
        "2. Certain combinations of disciplines, like computer science and biology, show more fruitful collaboration than others.\n",
        "\n",
        " Data to be Collected:\n",
        "To answer this question, I would need to gather data from scientific publications. Specifically:\n",
        "1. Title of the article: To identify the topics and fields involved.\n",
        "2. Author information: To analyze the collaboration between authors from different disciplines.\n",
        "3. Affiliation/Institution: To understand which institutions are promoting interdisciplinary research.\n",
        "4. Discipline/Field of study: To classify articles into specific disciplines.\n",
        "5. Year of publication: To identify trends over time.\n",
        "6. Citation count: To measure the impact of interdisciplinary versus single-discipline research.\n",
        "7. Abstract/Keywords: To use natural language processing (NLP) for deeper analysis of interdisciplinary topics.\n",
        "\n",
        "Potential Sources:\n",
        "- Semantic Scholar API: It offers rich metadata, including author information, keywords, abstracts, and citations.\n",
        "- Microsoft Academic Graph (MAG): This contains detailed information on papers, authors, and institutional affiliations.\n",
        "- Scopus/Dimensions: Paid databases that provide comprehensive metadata and citation data for articles, authors, and institutions.\n",
        "\n",
        " Amount of Data Needed:\n",
        "To ensure robust analysis, I'd aim to collect data on:\n",
        "- At least 10,000 articles** spanning the last 10 years (2014-2024) from a variety of fields (e.g., STEM, social sciences, humanities).\n",
        "- For interdisciplinary studies, the data should include at least **2,000 articles** from each of the top 5 most frequent discipline pairings (e.g., biology-computer science, economics-data science).\n",
        "\n",
        "A balanced dataset of interdisciplinary versus single-discipline studies will help compare their impacts and trends.\n",
        "\n",
        " Steps for Collecting the Data:\n",
        "\n",
        "Step 1: Define Fields and Discipline Classifications\n",
        "- First, create a list of discipline classifications (e.g., biology, computer science, social science, data science). You can base this on existing taxonomies like the **Field of Research Classification (FoR)** or **ACM Classification**.\n",
        "\n",
        "Step 2: Query APIs for Articles\n",
        "- Use the **Semantic Scholar API** and other relevant APIs (e.g., MAG, Scopus) to query articles based on keywords related to interdisciplinary collaboration, such as \"interdisciplinary,\" \"multidisciplinary,\" or combinations of discipline keywords (e.g., \"biology\" AND \"computer science\").\n",
        "\n",
        "Step 3: Collect Metadata for Each Article\n",
        "For each article, the following metadata should be collected:\n",
        "1. Title\n",
        "2. Authors\n",
        "3. Affiliations\n",
        "4. Year\n",
        "5. Citation count\n",
        "6. Discipline/Keywords\n",
        "7. Abstract\n",
        "\n",
        "Step 4: Automate Data Collectio\n",
        "\n",
        "Step 5: Process and Clean Data\n",
        "- Once data is collected, parse and clean it:\n",
        "  - Remove duplicates.\n",
        "  - Normalize the fields of study (e.g., map varying terminologies for \"computer science\" into a single category).\n",
        "  - Perform NLP analysis on abstracts and keywords to detect interdisciplinary topics.\n",
        "\n",
        "Step 6: Supplement Missing Data\n",
        "- For citation counts or author affiliations that might be missing, cross-reference with other databases (e.g., using MAG or Scopus APIs).\n",
        "\n",
        "Step 7: Save Data\n",
        "- Save the collected articles in **CSV or JSON format** for further analysis.\n",
        "\n",
        "\n",
        "Step 8: Analyze the Data\n",
        "- Use **NLP techniques** to identify emerging interdisciplinary fields.\n",
        "- Conduct **citation analysis** to compare the impact of interdisciplinary research versus single-discipline research.\n",
        "- Visualize trends over time, such as the number of interdisciplinary publications per year, top collaborating fields, and key institutions.\n",
        "\n",
        "Ethical Considerations:\n",
        "- Ensure that any API requests respect the platformâ€™s rate limits and terms of service.\n",
        "- The data used should be for academic purposes only, and if necessary, licenses or permissions should be obtained.\n",
        "\n",
        "This study could provide insights into how scientific collaboration is evolving and guide research policy on promoting interdisciplinary work.\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}